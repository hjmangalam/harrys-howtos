= HPC Description
by The HPC Support Staff
v1.0, Jan 06, 2015

//Harry Mangalam mailto:harry.mangalam@uci.edu[harry.mangalam@uci.edu]
// Convert this file to HTML & move to it's final dest with the command:
// export fileroot="/data/hpc/www/HPC_Overview"; asciidoc -a icons -a toc2 -b html5 -a numbered ${fileroot}.txt; scp ${fileroot}.[ht]* moo.nac.uci.edu:~/public_html
// export fileroot="/home/hjm/nacs/HPC_Overview"; asciidoc -a icons -a toc2 -b html5 -a numbered ${fileroot}.txt; scp ${fileroot}.[ht]* moo.nac.uci.edu:~/public_html

== What HPC is
The HPC facility is a 'Condo Cluster' available to all researchers at UCI (no
School or Dept affiliation required). 'Condo' means that most of the compute
nodes are individually owned by contributing Schools, Depts, or individual PIs,
and are available to them preferentially, but when not so used, are available to
the rest of campus users, using the http://hpc.oit.uci.edu/free-queue[Free Queue] system.
This allows all users to effectively get more computational power than they paid for, 
since usage among owners is quite spiky and the 'Free Queue' allows us to harvest 
otherwise unused cycles. 

While the HPC facility is not http://goo.gl/Qqcez[HIPAA-compliant], we have been told that
appropriately anonymized human data are being reduced and analyzed through
various pipelines on HPC.

In summary, HPC offers very large computing resources to all UCI researchers
to allow them to store and analyze data of a size that would not otherwise be possible locally,
and assistance in user-specific computational problems.

== Hardware
The HPC cluster is now the largest on campus with almost 5800 64bit CPU cores, \~36TB
aggregate RAM, 6 http://www.nvidia.com/object/tesla-servers.html[Nvidia Tesla GPUs], 
with Quad Data Rate (40Gbs) Infiniband interconnects. 
There is \~1PB of storage with 650TB in 2 http://www.beegfs.com/cms/[BeeGFS (aka Fraunhofer)] 
distributed filesystems, and the rest available from \~34 public and private NFS automounts.  
We have \~800 users of whom ~70 are logged in at any
one time on 2 login nodes and over 250 use the cluster in an average month. The cluster generally
has 65%-90% usage with several thousand jobs running and queued at any one time
(>6000 as I write this; 20,000 is not unusual).

== Software
The HPC staff can usually add requested Open Source Software within a
day; our 'module' system has >600 external applications (various versions),
libraries, and compilers/interpreters. These include image processing
(freesurfer/fsl), genomic analysis (bam/samtools, Bioconductor, BLAST, Annovar,
cufflinks), statistics (R, SAS, STATA) numerical processing (MATLAB,
Mathematica, Pylab/Numpy, Scilab, GSL, Boost, etc), molecular dynamics (Gromacs,
NAMD, AMBER, VMD), as well as a number of Engineering applications (OpenSees,
Openfoam). Our compilers support parallel approaches including OpenMP, OpenMPI,
Gnu Parallel, R's SNOW, and CUDA. We support debugging and profiling with 
TotalView, PerfSuite, HPCToolkit, Oprofile, and PAPI. We also support a local 
installation of Galaxy, integrated into our GridEngine scheduler.

Our module library currently (Jan 07, 2015) consists of 
http://hpc.oit.uci.edu/all.modules[this list].


== Classes
We run classes and tutorials in using Linux and the HPC cluster as well as
Bioinformatics (in conjunction with the 
http://ghtf.biochem.uci.edu/[Genomic High Throughput Facility]) and
also offer classes in 'BigData' processing (with Padraic Smith of
Information & Computer Sciences).

== Staff
HPC is supported by 2.5 FTEs, including 1.5 PhDs (in Molecular Biology &
Bioinformatics and in Signal Processing), as well as 1 student assistant and
these support staff regularly assist with debugging workflows and performance
problems in all domains (~700 domain-specific emails per month). We very much 
welcome new computational and research challenges and our interaction with 
scientists. Please feel free to mail all of us at <hpc-support@uci.edu>.

'Harry Mangalam', PhD, for the Research Computing Support Staff +
'Joseph Farran', Senior Sysadmin +
'Garr Updegraff', PhD, +
'Edward Xia' (student) +
